{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "661a21e8-12f7-4af2-ae74-b15d6bdd2ded",
   "metadata": {},
   "source": [
    "# Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71242a35-5712-4913-8ca4-c8c25a97ac62",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01914de2-0e7b-4b01-bc59-89aab92955d0",
   "metadata": {},
   "source": [
    "After learning about Adversarial Search in Artificial Intelligence and Game Theory, I wanted to dive deeper into its use in modern machine learning. That's when I discovered Generative Adversarial Networks (GANs).\n",
    "\n",
    "A GAN is a type of generative model that has two main parts: a Generative model (G) and a Discriminative model (D). These components interact with each other in a way that resembles a minimax game. The Discriminative model aims to maximize its value function, while the Generative model aims to minimize its value function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaac64db-6ac3-4425-b926-93bcb054949e",
   "metadata": {},
   "source": [
    "## Overview of GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caedc50c-6d2c-40c4-b7f3-cbd17009101f",
   "metadata": {},
   "source": [
    "As i mentioned earlier, the GAN consists of two parts. The generative model takes noisy variables $Z$ as input and maps them to the data space. This mapping can be represented by a differentiable function $G(z;\\theta_g) = x$, which can be implemented as a multilayer perceptron with parameters ùúÉùëî. On the other hand, the discriminative model's task is to determine whether inputs belong to the data distribution or not. It operates like a classifier, labeling inputs based on their likelihood of coming from the data distribution. The function $D(X; \\theta_d)$ represents the probability of $X$ belonging to the data distribution.\n",
    "\n",
    "Now, we can combine these two components. The generator's objective is to map random noisy inputs $Z$ to $X$ in such a way that the discriminator cannot detect that they are generated samples from ùëùùëî. At the same time, the discriminator aims to learn the most accurate model possible, enabling it to distinguish between real and fake data.\n",
    "\n",
    "After training this model, we will be able to generate data (such as images) that closely resemble real data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63c2b2b-577f-477a-b436-3af347fcc4c8",
   "metadata": {},
   "source": [
    "## Generator Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5370a5-8e34-4ab1-9732-6c9afc49c722",
   "metadata": {},
   "source": [
    "What the generator does is similar to finding a transformation function that minimizes an objective function. In mathematical terms, let's assume that $Z$ is drawn from a distribution called $P_z$. Our goal is to find a transformation function, denoted as $G$, that can accurately estimate the real data distribution called $P_{data}$. Essentially, the generator's objective is to make the discriminator unable to distinguish between real data from $P_{data}$ and fake data from $P_g$.\n",
    "\n",
    "To achieve this, we can define a suitable loss function for the generator using binary cross entropy : $argmin_{G} \\log(1 - D(G(Z)))$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce792b5-17b7-436b-944c-b77314481719",
   "metadata": {},
   "source": [
    "## Discriminator Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ef6ce0-c6f8-4f4f-bdfb-0e62e4484b62",
   "metadata": {},
   "source": [
    "The discriminator is responsible for labeling the inputs by determining if they belong to the data distribution. In simpler terms, the discriminator acts like a sigmoid function that outputs the probability of an input $x$ belonging to the real data distribution $P_{data}$. It is reasonable to use binary cross entropy as the loss function for the discriminator.\n",
    "\n",
    "However, due to the adversarial nature of this network, a new loss function can be considered for the discriminator. This new loss function focuses on the part of binary cross entropy that examines when the label $y$ is equal to 1. Based on our intuition from binary cross entropy and the role of the maximization node in minimax games, we can define the following objective function : $argmax_{G}\\log(D(X))$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8c5532-c96b-4216-8285-16a764676366",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba62d6c-a53f-4c91-ae9e-9474a49f8fba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "893ddf98-b0f8-4945-9fa8-9b638caff984",
   "metadata": {},
   "source": [
    "## Theoretical Formulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d650b928-5491-461f-832d-29d0141f101a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
